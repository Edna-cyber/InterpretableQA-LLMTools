{"qid": "hard-0001", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. Options: Reinforcement Learning, Deep Learning, Probabilistic Methods.", "answer": "Deep Learning"}
{"qid": "hard-0002", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference. Options: Optimization, Social Aspects, Theory.", "answer": "Social Aspects"}
{"qid": "hard-0003", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We consider Generative Adversarial Networks (GANs) and address the underlying functional optimization problem ab initio within a variational setting. Strictly speaking, the optimization of the generator and discriminator functions must be carried out in accordance with the Euler-Lagrange conditions, which become particularly relevant in scenarios where the  optimization cost involves regularizers comprising the derivatives of these functions. Considering Wasserstein GANs (WGANs) with a gradient-norm penalty, we show that the optimal discriminator is the solution to a Poisson differential equation. In principle, the optimal discriminator can be obtained in closed form without having to train a neural network. We illustrate this by employing a Fourier-series approximation to solve the Poisson differential equation. Experimental results based on synthesized Gaussian data demonstrate superior convergence behavior of the proposed approach in comparison with the baseline WGAN variants that employ weight-clipping, gradient or Lipschitz penalties on the discriminator on low-dimensional data. We also analyze the truncation error of the Fourier-series approximation  and the estimation error of the Fourier coefficients in a high-dimensional setting. We demonstrate applications to real-world images considering latent-space prior matching in Wasserstein autoencoders and present performance comparisons on  benchmark datasets such as MNIST, SVHN, CelebA, CIFAR-10, and Ukiyo-E. We demonstrate that the proposed approach achieves comparable reconstruction error and Frechet inception distance with faster convergence and up to two-fold improvement in image sharpness.. Title: Towards Label Position Bias in Graph Neural Networks. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0004", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: GLUTEN-FREE FLOUR BLEND, VANITY MIRROR? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0005", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: CONCEALER PLATE FOR A LIGHTING FIXTURE, TRANSPARENT DISPLAY DEVICE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0006", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We present a new combinatorial bandit model, the \\textit{cascading contextual assortment bandit}. This model serves as a generalization of both existing cascading bandits and assortment bandits, broadening their applicability in practice. For this model, we propose our first UCB bandit algorithm, UCB-CCA. We prove that this algorithm achieves a $T$-step regret upper-bound of $\\tilde{\\mathcal{O}}(\\frac{1}{\\kappa}d\\sqrt{T})$, sharper than existing bounds for cascading contextual bandits by eliminating dependence on cascade length $K$. To improve the dependence on problem-dependent constant $\\kappa$, we introduce our second algorithm, UCB-CCA+, which leverages a new Bernstein-type concentration result. This algorithm achieves $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ without dependence on $\\kappa$ in the leading term. We substantiate our theoretical claims with numerical experiments, demonstrating the practical efficacy of our proposed methods.. Title: Cascading Contextual Assortment Bandits. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0007", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems that adapt to disruptions by modifying their operation. We show conditions under which this balance can be achieved and introduce a practical algorithm to compute it, for which we derive approximation and generalization guarantees. We showcase the advantages of this resilient learning method in image classification tasks involving multiple potential invariances and in federated learning under distribution shift.. Title: Resilient Constrained Learning. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0008", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Options: Deep Learning, Applications, Social Aspects.", "answer": "Deep Learning"}
{"qid": "hard-0009", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time from 5 days to 5 hours on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods.. Title: RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0010", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Whenever a clinician reflects on the efficacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient's health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to retrospectively analyze sequential decision making processes, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A* algorithm that, under a natural form of Lipschitz continuity of the environment’s dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks.. Title: Finding Counterfactually Optimal Action Sequences in Continuous State Spaces. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0011", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: E-VAPING DEVICE HAVING A SECTION WITH A REMOVABLE INSULATOR BETWEEN ELECTRICALLY CONDUCTIVE AND PASSIVE ELEMENTS, DATA PROCESSING DEVICE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0012", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. Options: Applications, Social Aspects, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0013", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: High Pressure Fitting, EXPANSION RESERVOIR FOR A COOLANT CIRCUIT? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0014", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: COMPRESSOR MOTOR WITH CENTER STATOR, FUEL PUMP? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0015", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We consider Generative Adversarial Networks (GANs) and address the underlying functional optimization problem ab initio within a variational setting. Strictly speaking, the optimization of the generator and discriminator functions must be carried out in accordance with the Euler-Lagrange conditions, which become particularly relevant in scenarios where the  optimization cost involves regularizers comprising the derivatives of these functions. Considering Wasserstein GANs (WGANs) with a gradient-norm penalty, we show that the optimal discriminator is the solution to a Poisson differential equation. In principle, the optimal discriminator can be obtained in closed form without having to train a neural network. We illustrate this by employing a Fourier-series approximation to solve the Poisson differential equation. Experimental results based on synthesized Gaussian data demonstrate superior convergence behavior of the proposed approach in comparison with the baseline WGAN variants that employ weight-clipping, gradient or Lipschitz penalties on the discriminator on low-dimensional data. We also analyze the truncation error of the Fourier-series approximation  and the estimation error of the Fourier coefficients in a high-dimensional setting. We demonstrate applications to real-world images considering latent-space prior matching in Wasserstein autoencoders and present performance comparisons on  benchmark datasets such as MNIST, SVHN, CelebA, CIFAR-10, and Ukiyo-E. We demonstrate that the proposed approach achieves comparable reconstruction error and Frechet inception distance with faster convergence and up to two-fold improvement in image sharpness.. Title: Euler-Lagrange Analysis of Generative Adversarial Networks. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0016", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Certifiable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use \\emph{likelihood ratios} to construct \\emph{any-time valid} confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a \\emph{non-asymptotic} analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions.. Title: Likelihood Ratio Confidence Sets for Sequential Decision Making. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0017", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The prosperity of deep neural networks (DNNs) is largely benefited from open-source datasets, based on which users can evaluate and improve their methods. In this paper, we revisit backdoor-based dataset ownership verification (DOV), which is currently the only feasible approach to protect the copyright of open-source datasets. We reveal that these methods are fundamentally harmful given that they could introduce malicious misclassification behaviors to watermarked DNNs by the adversaries. In this paper, we design DOV from another perspective by making watermarked models (trained on the protected dataset) correctly classify some `hard' samples that will be misclassified by the benign model. Our method is inspired by the generalization property of DNNs, where we find a \\emph{hardly-generalized domain} for the original dataset (as its \\emph{domain watermark}). It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and propose to optimize a set of visually-indistinguishable clean-label modified data with similar effects to domain-watermarked samples from the hardly-generalized domain to ensure watermark stealthiness. We also design a hypothesis-test-guided ownership verification via our domain watermark and provide the theoretical analyses of our method. Extensive experiments on three benchmark datasets are conducted, which verify the effectiveness of our method and its resistance to potential adaptive methods.. Title: Explainable and Efficient Randomized Voting Rules. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0018", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Immunoassay Test Slide, LOCATION SYSTEM AND COMPUTER PROGRAM? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0019", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.. Title: Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0020", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: FILM CUTTER ASSEMBLY, AUTHENTICATION-FREE CONFIGURATION FOR SERVICE CONTROLLERS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0021", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. Options: Theory, Deep Learning, Reinforcement Learning.", "answer": "Deep Learning"}
{"qid": "hard-0022", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: The Quantization Model of Neural Scaling. Options: Applications, Deep Learning, Optimization.", "answer": "Deep Learning"}
{"qid": "hard-0023", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Knowledge distillation (KD) has emerged as an effective technique for compressing models that can enhance the lightweight model.  Conventional KD methods propose various designs to allow student model to imitate the teacher better.  However, these handcrafted KD designs heavily rely on expert knowledge and may be sub-optimal for various teacher-student pairs.  In this paper, we present a novel framework, KD-Zero, which utilizes evolutionary search to automatically discover promising distiller  from scratch for any teacher-student architectures.  Specifically, we first decompose the generalized distiller into knowledge transformations, distance functions, and loss weights.  Then,  we construct our distiller search space by selecting advanced operations for these three components.  With sharpness and represent gap as fitting objectives, we evolve candidate populations and generate better distillers by crossover and mutation.  To ensure efficient searching, we employ the loss-rejection protocol, search space shrinkage, and proxy settings during the search process.  In this manner, the discovered distiller can address the capacity gap and cross-architecture challenges for any teacher-student pairs in the final distillation stage.  Comprehensive experiments reveal that KD-Zero consistently outperforms other state-of-the-art methods across diverse architectures on classification, detection, and segmentation tasks.  Noticeably, we provide some practical insights in designing the distiller by analyzing the distiller discovered.  Codes are available in supplementary materials.. Title: Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0024", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Options: Probabilistic Methods, Social Aspects, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0025", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: IN SITU HEAT INDUCED ANTIGEN RECOVERY AND STAINING APPARATUS AND METHOD, Method, System, and Apparatus for RF Switching Amplifier? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0026", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos. Options: Applications, Deep Learning, Theory.", "answer": "Applications"}
{"qid": "hard-0027", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEMS AND METHODS FOR INTERCHANGABLE ADDITIVE MANUFACTURING SYSTEMS, STORE VISIT DATA CREATION AND MANAGEMENT? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0028", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency. Options: Deep Learning, Applications, Theory.", "answer": "Applications"}
{"qid": "hard-0029", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Conditional Distribution Function Estimation Using Neural Networks for Censored and Uncensored Data. Options: Theory, Optimization, Deep Learning.", "answer": "Theory"}
{"qid": "hard-0030", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: CENTRIFUGAL CLUTCH, Magnetic Clamp? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0031", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Device and Method for Blind Control and Automation, WINDOW TREATMENT HAVING AN ADJUSTABLE BOTTOM BAR? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0032", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Does Invariant Graph Learning via Environment Augmentation Learn Invariance?. Options: Optimization, Reinforcement Learning, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0033", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's functional equivalence class is the set of parameters implementing the same input--output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of reducible parameters, with richer functional equivalence classes caused by redundancies among the network's units.In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.. Title: Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0034", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Scope of Reproducibility: The paper presents a novel post-hoc regularization technique for tree-based models, called Hierarchical Shrinkage (Agarwal 2022). Our main goal is to confirm the claims that it substantially increases the predictive performance of both decision trees and random forests, that it is faster than other regularization techniques, and that it makes the interpretation of random forests simpler.\nMethodology: In our reproduction, we used the Hierarchical Shrinkage, provided by the authors in the Python package imodels. We also used their function for obtaining pre-cleaned data sets. While the algorithm code and clean datasets were provided we re-implemented the experiments as well as added additional experiments to further test the validity of the claims. The results were tested by applying Hierarchical Shrinkage to different tree models and comparing them to the authors' results.\nResults: We managed to reproduce most of the results the authors get. The method works well and most of the claims are supported. The method does increase the predictive performance of tree-based models most of the time, but not always. When compared to other regularization techniques the Hierarchical Shrinkage outperforms them when used on decision trees, but not when used on random forests. Since the method is applied after learning, it is extremely fast. And it does simplify decision boundaries for random forests, making them easier to interpret.\nWhat was easy: The use of the official code for Hierarchical Shrinkage was straightforward and used the same function naming convention as other machine learning libraries. The function for acquiring already clean data sets saved a lot of time.\nWhat was difficult: The authors also provided the code for their experiments in a separate library, but the code did not run out of the box and we had no success reproducing the results with it. The code was inconsistent with the paper methodology. We had the most problems with hyperparameter tuning. The authors did not specify how they tuned the hyperparameters for the used RF regularizers.\nCommunication with original authors: We did not contact the authors of the original paper. Title: [Re] Hierarchical Shrinkage: Improving the Accuracy and Interpretability of Tree-Based Methods. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0035", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In this work we study statistical properties of graph-based algorithms for multi-manifold clustering (MMC). In MMC the goal is to retrieve the multi-manifold structure underlying a given Euclidean data set when this one is assumed to be obtained by sampling a distribution on a union of manifolds $\\M = \\M_1 \\cup\\dots  \\cup \\M_N$ that may intersect with each other and that may have different dimensions. We investigate sufficient conditions that similarity graphs on data sets must satisfy in order for their corresponding graph Laplacians to capture the right geometric information to solve the MMC problem. Precisely, we provide high probability error bounds for the spectral approximation of a tensorized Laplacian on $\\M$ with a suitable graph Laplacian built from the observations; the recovered tensorized Laplacian contains all geometric information of all the individual underlying manifolds. We provide an example of a family of similarity graphs, which we call annular proximity graphs with angle constraints, satisfying these sufficient conditions. We contrast our family of graphs with other constructions in the literature based on the alignment of tangent planes. Extensive numerical experiments expand the insights that our theory provides on the MMC problem.. Title: Large sample spectral analysis of graph-based multi-manifold clustering. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0036", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PRINTING AGENT, PRINTING AGENT PRODUCTION METHOD, AND CLOTH ARTICLE, NETWORK NODE, WIRELESS DEVICE AND METHODS THEREIN FOR PERFORMING AND HANDLING SUPERPOSED TRANSMISSIONS IN A WIRELESS COMMUNICATIONS NETWORK? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0037", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing. Options: Applications, Optimization, Deep Learning.", "answer": "Applications"}
{"qid": "hard-0038", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEM AND METHOD FOR PROACTIVELY OFFERING FINANCING OFFERS TO CUSTOMERS OF E-COMMERCE WEBSITES, SHOPPING ASSISTANT? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0039", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: REMIX CAPACITY MANAGEMENT FOR OPTICAL ORDERS, ANTI-STATIC PACKAGE FOR MEDICAL CONTAINERS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0040", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Diffusion models have achieved remarkable success in diverse domains such as image synthesis, super-resolution, and 3D molecule generation. Surprisingly, the application of diffusion models in graph learning has garnered little attention. In this paper, we aim to bridge this gap by exploring the use of diffusion models for unsupervised graph representation learning. Our investigation commences with the identification of anisotropic structures within graphs and the recognition of a crucial limitation in the vanilla forward diffusion process when dealing with these anisotropic structures. The original forward diffusion process continually adds  isotropic Gaussian noise to the data, which may excessively dilute anisotropic signals, leading to rapid signal-to-noise conversion. This rapid conversion poses challenges for training denoising neural networks and obstructs the acquisition of semantically meaningful representations during the reverse process. To overcome this challenge, we introduce a novel class of models termed {\\it directional diffusion models}.  These models adopt data-dependent, anisotropic, and directional noises in the forward diffusion process. In order to assess the effectiveness of our proposed models, we conduct extensive experiments on 12 publicly available datasets, with a particular focus on two distinct graph representation learning tasks. The experimental results unequivocally establish the superiority of our models over state-of-the-art baselines, underscoring their effectiveness in capturing meaningful graph representations. Our research not only sheds light on the intricacies of the forward process in diffusion models but also underscores the vast potential of these models in addressing a wide spectrum of graph-related tasks. Our code is available at \\url{https://github.com/statsle/DDM}.. Title: Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0041", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: 4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision that MM-Fi can contribute to wireless sensing research with respect to action recognition, human pose estimation, multi-modal learning, cross-modal supervision, and interdisciplinary healthcare research.. Title: MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0042", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pretrained from scratch on the C4 dataset, this base model achieves a downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed Pareto curves and show that MosaicBERT base and large are consistently Pareto optimal when compared to a competitive BERT base and large. This empirical speed up in pretraining enables researchers and engineers to pretrain custom BERT-style models at low cost instead of finetune on existing generic models. We open source our model weights and code.. Title: MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0043", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Audio-visual segmentation is a challenging task that aims to predict pixel-level masks for sound sources in a video.  Previous work applied a comprehensive manually designed architecture with countless pixel-wise accurate masks as supervision.  However, these pixel-level masks are expensive and not available in all cases.  In this work, we aim to simplify the supervision as the instance-level annotation, $\\textit{i.e.}$, weakly-supervised audio-visual segmentation.  We present a novel Weakly-Supervised Audio-Visual Segmentation framework, namely WS-AVS, that can learn multi-scale audio-visual alignment with multi-scale multiple-instance contrastive learning for audio-visual segmentation.  Extensive experiments on AVSBench demonstrate the effectiveness of our WS-AVS in the weakly-supervised audio-visual segmentation of single-source and multi-source scenarios.. Title: Pairwise Causality Guided Transformers for Event Sequences. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0044", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation. Options: Applications, Reinforcement Learning, Social Aspects.", "answer": "Applications"}
{"qid": "hard-0045", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: QLoRA: Efficient Finetuning of Quantized LLMs. Options: Social Aspects, Reinforcement Learning, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0046", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Electrical Cord Having Plugs With Improved Safety Features, STABILIZED RAILWAY FREIGHT CAR TRUCK? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0047", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic.However, TD-learning updates can be high variance.Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates.This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update.We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption.Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3.We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based baseline algorithms on a set of standard benchmark tasks.. Title: Taylor TD-learning. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0048", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Visual object tracking (VOT) is one of the most fundamental tasks in computer vision community. State-of-the-art VOT trackers extract positive and negative examples that are used to guide the tracker to distinguish the object from the background. In this paper, we show that this characteristic can be exploited to introduce new threats and hence propose a simple yet effective poison-only backdoor attack. To be specific, we poison a small part of the training data by attaching a predefined trigger pattern to the background region of each video frame, so that the trigger appears almost exclusively in the extracted negative examples. To the best of our knowledge, this is the first work that reveals the threat of poison-only backdoor attack on VOT trackers. We experimentally show that our backdoor attack can significantly degrade the performance of both two-stream Siamese and one-stream Transformer trackers on the poisoned data while gaining comparable performance with the benign trackers on the clean data.. Title: Weighted ROC Curve in Cost Space: Extending AUC to Cost-Sensitive Learning. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0049", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power), or arbitrary kernel choices such as the median heuristic. We prove that MMDAgg still controls the level non-asymptotically, and achieves the minimax rate over Sobolev balls, up to an iterated logarithmic term. Our guarantees are not restricted to a specific type of kernel, but hold for any product of one-dimensional translation invariant characteristic kernels. We provide a user-friendly parameter-free implementation of MMDAgg using an adaptive collection of bandwidths. We demonstrate that MMDAgg significantly outperforms alternative state-of-the-art MMD-based two-sample tests on synthetic data satisfying the Sobolev smoothness assumption, and that, on real-world image data, MMDAgg closely matches the power of tests leveraging the use of models such as neural networks.. Title: MMD Aggregated Two-Sample Test. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0050", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: MOUNTING DEVICE FOR EXPANSION CARD, DUST-PROOF MECHANISM AND ELECTRONIC DEVICE USING THE SAME? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0051", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.. Title: On Masked Pre-training and the Marginal Likelihood. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0052", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: TOA: Task-oriented Active VQA. Options: Social Aspects, Applications, Probabilistic Methods.", "answer": "Applications"}
{"qid": "hard-0053", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PACKAGE SYSTEM FOR INTEGRATED CIRCUITS, SEMICONDUCTOR INTEGRATED CIRCUIT DEVICE HAVING A STANDARD CELL WHICH INCLUDES A FIN? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0054", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PROVIDING FINANCIAL TRANSACTION DATA TO A USER, WAVE DISSECTING AND REDIRECTING EQUIPMENT AND SYSTEM TO REDIRECT WAVES AWAY FROM COASTAL AREAS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0055", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Interpretable Prototype-based Graph Information Bottleneck. Options: Deep Learning, Optimization, Applications.", "answer": "Deep Learning"}
{"qid": "hard-0056", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: A POLYMERIC CARRIER FOR DELIVERY OF A PAYLOAD TO A CELL, BLOCK COPOLYMER HAVING PHENYLBORONIC ACID GROUP INTRODUCED THEREIN, AND USE THEREOF? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0057", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art (SOTA) sampling methods for few-step sampling; 2) SOTA FID on substantial benchmark datasets under a suitable number of function evaluations (NFEs).. Title: Convex-Concave Zero-Sum Markov Stackelberg Games. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0058", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SUSPENSION MOUNT, Determining Weight of a Vehicle in Reverse Gear? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0059", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related biases within a hegemonic system of binary gender, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between pronoun resolution accuracies for image subjects with gender presentations perceived as masculine versus feminine by human annotators and ii) retrieval bias, where we compare ratios of professionals perceived to have masculine and feminine gender presentations retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they demonstrate bias in resolving binary gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models are generally less biased than Vision-Language Encoders.. Title: Efficient Training of Energy-Based Models Using Jarzynski Equality. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0060", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: METHOD AND COMPOSITION FOR TREATING A SEROTONIN RECEPTOR-MEDIATED CONDITION, EXHALATION PORT? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0061", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SILANE-BASED COMPOUND AND ORGANIC LIGHT-EMITTING DEVICE INCLUDING THE SAME, HETEROJUNCTION BIPOLAR TRANSISTOR? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0062", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Hardware-specific optimizations in machine learning (ML) frameworks can cause numerical deviations of inference results. Quite surprisingly, despite using a fixed trained model and fixed input data, inference results are not consistent across platforms, and sometimes not even deterministic on the same platform. We study the causes of these numerical deviations for convolutional neural networks (CNN) on realistic end-to-end inference pipelines and in isolated experiments. Results from 75 distinct platforms suggest that the main causes of deviations on CPUs are differences in SIMD use, and the selection of convolution algorithms at runtime on GPUs. We link the causes and propagation effects to properties of the ML model and evaluate potential mitigations. We make our research code publicly available.. Title: Regret Minimization via Saddle Point Optimization. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0063", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Deep Equilibrium Based Neural Operators for Steady-State PDEs. Options: Theory, Applications, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0064", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: HOSE REEL SYSTEMS, PICKUP ROLLER HAVING FRONT END ALIGNMENT MEMBER AND MEDIUM SEPARATING DEVICE USING SAME? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0065", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.. Title: Temporal Continual Learning with Prior Compensation for Human Motion Prediction. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0066", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We derive upper bounds for random design linear regression with dependent ($\\beta$-mixing) data absent any realizability assumptions.  In contrast to the strictly realizable martingale noise regime, no sharp \\emph{instance-optimal} non-asymptotics are available in the literature. Up to constant factors, our analysis correctly recovers the variance term predicted by the Central Limit Theorem---the noise level of the problem---and thus exhibits graceful degradation as we introduce misspecification. Past a burn-in, our result is sharp in the moderate deviations regime, and in particular does not inflate the leading order term by mixing time factors.. Title: Uncovering and Quantifying Social Biases in Code Generation. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0067", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Compositional Explanations is a method for identifying logical formulas of concepts that approximate the neurons' behavior. However, these explanations are linked to the small spectrum of neuron activations (i.e., the highest ones) used to check the alignment, thus lacking completeness. In this paper, we propose a generalization, called Clustered Compositional Explanations, that combines  Compositional Explanations with clustering and a novel search heuristic to approximate a broader spectrum of the neuron behavior. We define and address the problems connected to the application of these methods to multiple ranges of activations, analyze the insights retrievable by using our algorithm, and propose desiderata qualities that can be used to study the explanations returned by different algorithms.. Title: Adversarially Robust Learning with Uncertain Perturbation Sets. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0068", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely $(G,B)$-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction $\\frac{1}{2}$. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysis reduces the gap between theory and practice.. Title: MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0069", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to match structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural datasets and find that the synapses learn optimal configurations over long timescales that enable adaptive whitening on short timescales using gain modulation.. Title: Adaptive whitening with fast gain modulation and slow synaptic plasticity. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0070", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: A Combinatorial Algorithm for Approximating the Optimal Transport in the Parallel and MPC Settings. Options: Applications, Optimization, Reinforcement Learning.", "answer": "Optimization"}
{"qid": "hard-0071", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEM AND METHOD FOR PROCESSING OBJECTS HAVING CONTAMINATING PARTICLES, Runtime Adjustment of Configuration Models for Consistency Preservation? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0072", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling.That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning.Under this formulation, the continual learning process becomes the forward pass of a sequence model.By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes.As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods.Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.. Title: Recasting Continual Learning as Sequence Modeling. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0073", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Medical Robotic System with Remote Current Controller for Controlling a Plurality of Distally Housed Motors, COSMETIC COMPOSITION FOR FACE AND THE AREA AROUND THE EYES? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0074", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Implicit Transfer Operator Learning: Multiple Time-Resolution Models for Molecular Dynamics. Options: Theory, Applications, Probabilistic Methods.", "answer": "Applications"}
{"qid": "hard-0075", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Public Opinion Field Effect Fusion in Representation Learning for Trending Topics Diffusion. Options: Deep Learning, Theory, Applications.", "answer": "Deep Learning"}
{"qid": "hard-0076", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We study the best arm identification problem in combinatorial semi-bandits in the fixed confidence setting. We present Perturbed Frank-Wolfe Sampling (P-FWS), an algorithm that (i) runs in polynomial time, (ii) achieves the instance-specific minimal sample complexity in the high confidence regime, and (iii) enjoys polynomial sample complexity guarantees in the moderate confidence regime. To our best knowledge, existing algorithms cannot achieve (ii) and (iii) simultaneously in vanilla bandits. With P-FWS, we close the computational-statistical gap in best arm identification in combinatorial semi-bandits. The design of P-FWS starts from the optimization problem that defines the information-theoretical and instance-specific sample complexity lower bound. P-FWS solves this problem in an online manner using, in each round, a single iteration of the Frank-Wolfe algorithm. Structural properties of the problem are leveraged to make the P-FWS successive updates computationally efficient. In turn, P-FWS only relies on a simple linear maximization oracle.. Title: Closing the Computational-Statistical Gap in Best Arm Identification for Combinatorial Semi-bandits. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0077", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: BATTERY MODULE, DISPLAY DEVICE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0078", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods often assume weights are treated solely based on value, neglecting the unique role of weight position. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster center and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialization setting and a regularization term, enabling the training of BNNs with complex dataset-model combinations. Leveraging the flexibility of weight values from probability distributions, we enhance noise resilience and compressibility. Our iterative clustering procedure demonstrates superior compressibility and higher accuracy compared to state-of-the-art methods on both ResNet models and the more complex transformer-based architectures. In particular, our method outperforms the state-of-the-art quantization method top-1 accuracy by 1.6\\% on ImageNet using DeiT-Tiny, with its 5 million+ weights now represented by only 296 unique values.  Code available at https://github.com/subiawaud/PWFN.. Title: A Variational Perspective on High-Resolution ODEs. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0079", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: METHODS AND APPARATUS FOR CHANGING MOBILE TELEPHONE OPERATION MODE BASED ON VEHICLE OPERATION STATUS, DEVICES AND METHODS FOR DETERMINING AND/OR ISOLATING CELLS SUCH AS CIRCULATING CANCER OR FETAL CELLS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0080", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Method Of Soil Conditioning By Application Of Water-Soluble Or Water-Swelling Polymer, PROPPANT SAND COATING FOR DUST REDUCTION? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0081", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: BOROSILICATE GLASS FOR PHARMACEUTICAL CONTAINER, GLASS TUBE FOR PHARMACEUTICAL CONTAINER, AND MANUFACTURING METHOD FOR PHARMACEUTICAL CONTAINER, METHOD FOR DETERMINING A DEPRESSION STATE AND DEPRESSION STATE DETERMINATION DEVICE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0082", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. Options: Reinforcement Learning, Probabilistic Methods, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0083", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions. Options: Probabilistic Methods, Applications, Reinforcement Learning.", "answer": "Applications"}
{"qid": "hard-0084", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Prediction of Fluid Composition and/or Phase Behavior, FERROELECTRIC MEMORY AND METHODS OF FORMING THE SAME? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0085", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: STREAMING DISPLAY DATA FROM A MOBILE DEVICE USING BACKSCATTER  COMMUNICATIONS, LOCAL CHANNELS ANYWHERE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0086", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associated with loss gradients tending toward infinity or zero. We then introduce a novel heuristic algorithm for detecting all fixed points and $k$-cycles in ReLU-based RNNs and their existence and stability regions, hence bifurcation manifolds in parameter space. In contrast to previous numerical algorithms for finding fixed points and common continuation methods, our algorithm provides $\\textit{exact}$ results and returns fixed points and cycles up to high orders with surprisingly good scaling behavior. We exemplify the algorithm on the analysis of the training process of RNNs, and find that the recently introduced technique of generalized teacher forcing completely avoids certain types of bifurcations in training. Thus, besides facilitating the DST analysis of trained RNNs, our algorithm provides a powerful instrument for analyzing the training process itself.. Title: Bifurcations and loss jumps in RNN training. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0087", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Motivated by a recent literature on the double-descent phenomenon in machine learning, we consider highly over-parameterized models in causal inference, including synthetic control with many control units. In such models, there may be so many free parameters that the model fits the training data perfectly. We first investigate high-dimensional linear regression for imputing wage data and estimating average treatment effects, where we find that models with many more covariates than sample size can outperform simple ones. We then document the performance of high-dimensional synthetic control estimators with many control units. We find that adding control units can help improve imputation performance even beyond the point where the pre-treatment fit is perfect. We provide a unified theoretical perspective on the performance of these high-dimensional models. Specifically, we show that more complex models can be interpreted as model-averaging estimators over simpler ones, which we link to an improvement in average performance. This perspective yields concrete insights into the use of synthetic control when control units are many relative to the number of pre-treatment periods.. Title: Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0088", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Latent Variable Models (LVMs) propose to model the dynamics of neural populations by capturing low-dimensional structures that represent features involved in neural activity. Recent LVMs are based on deep learning methodology where a deep neural network is trained to reconstruct the same neural activity given as input and as a result to build the latent representation. Without taking past or future activity into account such a task is non-causal. In contrast, the task of forecasting neural activity based on given input extends the reconstruction task. LVMs that are trained on such a task could potentially capture temporal causality constraints within its latent representation. Forecasting has received less attention than reconstruction due to recording challenges such as limited neural measurements and trials. In this work, we address modeling neural population dynamics via the forecasting task and improve forecasting performance by including a prior, which consists of pairwise neural unit interaction as a multivariate dynamic system. Our proposed model---Additive, Multiplicative, and Adaptive Graph Neural Network (AMAG)---leverages additive and multiplicative message-passing operations analogous to the interactions in neuronal systems and adaptively learns the interaction among neural units to forecast their future activity. We demonstrate the advantage of AMAG compared to non-GNN based methods on synthetic data and multiple modalities of neural recordings (field potentials from penetrating electrodes or surface-level micro-electrocorticography) from four rhesus macaques. Our results show the ability of AMAG to recover ground truth spatial interactions and yield estimation for future dynamics of the neural population.. Title: AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neuron Activity. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0089", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: GARDEN FOUNTAIN, WIND TURBINE FLUID APPLICATION APPARATUS? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0090", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: END OF LIFE DETECTION FOR ANALYTE SENSORS, Portable Iontophoretic System Using Sheet Mask? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0091", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Due to its promising performance, deep hashing has become a prevalent method for approximate nearest neighbors search (ANNs). However, most of current deep hashing methods are validated on relatively small-scale datasets, leaving potential threats when are applied to large-scale real-world scenarios. Specifically, they can be constrained either by the computational cost due to the large number of training categories and samples, or unsatisfactory accuracy. To tackle those issues, we propose a novel deep hashing framework based on product quantization (PQ). It uses a softmax-based differentiable PQ branch to learn a set of predefined PQ codes of the classes. Our method is easy to implement, does not involve large-scale matrix operations, and learns highly discriminate compact codes. We validate our method on multiple large-scaled datasets, including ImageNet100, ImageNet1K, and Glint360K, where the category size scales from 100 to 360K and sample number scales from 10K to 17 million, respectively. Extensive experiments demonstrate the superiority of our method. Code is available at https://github.com/yuleung/FPPQ.. Title: Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0092", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: [Re] Fairness Guarantees under Demographic Shift. Options: Probabilistic Methods, Social Aspects, Deep Learning.", "answer": "Social Aspects"}
{"qid": "hard-0093", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: A BURNER EVAPORATOR FOR A FUEL CELL SYSTEM, STRUCTURE AND METHOD FOR COMPRESSIVELY STRAINED SILICON GERMANIUM FINS FOR pFET DEVICES AND TENSILY STRAINED SILICON FINS FOR nFET DEVICES? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0094", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL) gradually lose their plasticity, the ability to learn from new data; however, the analysis and mitigation of this phenomenon is hampered by the complex relationship between plasticity, exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions. The applications of this intervention are two-fold: first, as a diagnostic tool — if injection increases the performance, we may conclude that an agent's network was losing its plasticity. This tool allows us to identify a subset of Atari environments where the lack of plasticity causes performance plateaus, motivating future studies on understanding and combating plasticity loss. Second, plasticity injection can be used to improve the computational efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or by growing the agent's network dynamically without compromising performance. The results on Atari show that plasticity injection attains stronger performance compared to alternative methods while being computationally efficient.. Title: NAS-X: Neural Adaptive Smoothing via Twisting. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0095", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The quest for human imitative AI has been an enduring topic in AI research since inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's `human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli --- distractors dubbed red herrings --- impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect's Connecting Wall segment essentially mimics Mednick's Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, that makes it an ideal proxy dataset to explore and study fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In addition to presenting the novel Only Connect Wall (OCW) dataset, we also report results from our evaluation of selected pre-trained language models and LLMs (including OpenAI's GPT series) on creative problem solving tasks like grouping clue words by heterogeneous connections, and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models.The code and link to the dataset is available at url.. Title: Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0096", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Simultaneous behavioral and electrophysiological recordings call for new methods to reveal the interactions between neural activity and behavior. A milestone would be an interpretable model of the co-variability of spiking activity and behavior across trials. Here, we model a mouse cortical sensory-motor pathway in a tactile detection task reported by licking with a large recurrent spiking neural network (RSNN), fitted to the recordings via gradient-based optimization. We focus specifically on the difficulty to match the trial-to-trial variability in the data. Our solution relies on optimal transport to define a distance between the distributions of generated and recorded trials. The technique is applied to artificial data and neural recordings covering six cortical areas. We find that the resulting RSNN can generate realistic cortical activity and predict jaw movements across the main modes of trial-to-trial variability. Our analysis also identifies an unexpected mode of variability in the data corresponding to task-irrelevant movements of the mouse.. Title: Trial matching: capturing variability with data-constrained spiking neural networks. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0097", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: DEVICE ENABLING EXCHANGE OF ABNORMAL SIGNAL AMONG VEHICLES VIA WI-FI DIRECT NETWORK, AND CONTROL METHOD THEREFOR, FLIGHT CONTROL APPARATUS AND UNMANNED AERIAL VEHICLE EQUIPPED WITH SAME? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0098", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: This paper presents a new mechanism to facilitate the training of mask transformers for efficient panoptic segmentation, democratizing its deployment. We observe that due to the high complexity in the training objective of panoptic segmentation, it will inevitably lead to much higher penalization on false positive. Such unbalanced loss makes the training process of the end-to-end mask-transformer based architectures difficult, especially for efficient models. In this paper, we present ReMaX that adds relaxation to mask predictions and class predictions during the training phase for panoptic segmentation. We demonstrate that via these simple relaxation techniques during training, our model can be consistently improved by a clear margin without any extra computational cost on inference. By combining our method with efficient backbones like MobileNetV3-Small, our method achieves new state-of-the-art results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes. Code and pre-trained checkpoints will be available at https://github.com/google-research/deeplab2.. Title: Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0099", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. Options: Optimization, Theory, Deep Learning.", "answer": "Optimization"}
{"qid": "hard-0100", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: WIDE FIELD PERSONAL DISPLAY, DENITRATION CATALYST COMPOSITION AND METHOD OF DENITRATION USING SAME? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0101", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Experimentation has been critical and increasingly popular  across various domains, such as clinical trials and online platforms, due to its widely recognized benefits. One of the primary objectives of classical experiments is to estimate the average treatment effect (ATE) to inform future decision-making. However, in healthcare and many other settings, treatment effects may be non-stationary, meaning that they can change over time, rendering the traditional experimental design inadequate and the classical static ATE uninformative. In this work, we address the problem of non-stationary experimental design under linear trends by considering two objectives: estimating the dynamic treatment effect and minimizing welfare loss within the experiment. We propose an efficient design that can be customized for optimal estimation error rate, optimal regret rate, or the Pareto optimal trade-off between the two objectives. We establish information-theoretical lower bounds that highlight the inherent challenge in estimating dynamic treatment effects and minimizing welfare loss, and also statistically reveal the fundamental trade-off between them.. Title: Large sample spectral analysis of graph-based multi-manifold clustering. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0102", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Nanophotonic structures have versatile applications including solar cells, anti-reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes. To design and understand these nanophotonic structures, electrodynamic simulations are essential. These simulations enable us to model electromagnetic fields over time and calculate optical properties. In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems. The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties. Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs.. Title: Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0103", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Towards a fuller understanding of neurons with Clustered Compositional Explanations. Options: Theory, Social Aspects, Probabilistic Methods.", "answer": "Social Aspects"}
{"qid": "hard-0104", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PHOTOELECTRIC CONVERSION ELEMENT, IMAGING ELEMENT, OPTICAL SENSOR, AND COMPOUND, Unmanned Aerial Vehicle Interactive Apparatus and Method Based on Deep Learning Posture Estimation? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0105", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Optical Sensing Device To Sense Displacement, SYSTEM AND METHOD FOR CREATING VIRTUAL DISK IMAGES FOR USE WITH REMOTE COMPUTER? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0106", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. Options: Deep Learning, Social Aspects, Applications.", "answer": "Applications"}
{"qid": "hard-0107", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Dynamic Non-monotone Submodular Maximization. Options: Applications, Optimization, Theory.", "answer": "Optimization"}
{"qid": "hard-0108", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: ANAMORPHOTIC TELESCOPE, Mirror For Solar-Skypipe Collector? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0109", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Online (Multinomial) Logistic Bandit: Improved Regret and Constant Computation Cost. Options: Optimization, Miscellaneous Aspects of Machine Learning, Applications.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0110", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. One increasingly common way to annotate documents cheaply at scale is through large language models (LLMs). However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. We present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical properties—like asymptotic unbiasedness and proper uncertainty quantification—which are fundamental to CSS research. We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90\\%. To address this, we build on debiased machine learning to propose the design-based supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.. Title: Counterfactually Comparing Abstaining Classifiers. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0111", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Methods, Systems and Computer Program Products for Dynamic Optical Histology Using Optical Coherence Tomography, VIBRATION DAMPER FOR A MOTOR VEHICLE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0112", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Reinforcement learning (RL) has shown great promise for developing agents for dialogue management (DM) that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite the advancements in RL and language models (LMs), employing RL to drive conversational chatbots still poses significant challenges. A primary issue stems from RL’s dependency on online exploration for effective learning, a process that can be costly. Moreover, engaging in online interactions with humans during the training phase can raise safety concerns, as the LM can potentially generate unwanted outputs. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop various RL algorithms, specialized in dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs)---models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting the MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM. We evaluate our methods in open-domain dialogue to demonstrate their effectiveness with respect to the diversity of intent in generated utterances and overall DM performance.. Title: Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0113", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Standard diffusion models involve an image transform  -- adding Gaussian noise -- and an image restoration operator that inverts this degradation.  We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact, an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference and paves the way for generalized diffusion models that invert arbitrary processes.. Title: Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0114", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Maximum Independent Set: Self-Training through Dynamic Programming. Options: Probabilistic Methods, Deep Learning, Social Aspects.", "answer": "Deep Learning"}
{"qid": "hard-0115", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: DEVICE FOR POSITIONING PLATEN ROLLER, INKJET PRINTHEAD HAVING PRINTHEAD CHIPS ATTACHED TO TRUSS STRUCTURE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0116", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Scope of ReproducibilityIn this work, we evaluate the reproducibility of the paper Label-Free Explainability for Unsupervised Models by Crabbe and van der Schaar. Our goal is to reproduce the paper's four main claims in a label-free setting:(1) feature importance scores determine salient features of a model's input, (2) example importance scores determine salient training examples to explain a test example, (3) interpretability of saliency maps is hard for disentangled VAEs, (4) distinct pretext tasks don’t have interchangeable representations.MethodologyThe authors of the paper provide an implementation in PyTorch for their proposed techniques and experiments. We reuse and extend their code for our additional experiments. Our reproducibility study comes at a total computational cost of 110 GPU hours, using an NVIDIA Titan RTX. ResultsWe reproduced the original paper's work through our experiments. We find that the main claims of the paper largely hold. We assess the robustness and generalizability of some of the claims, through our additional experiments. In that case, we find that one claim is not generalizable and another is not reproducible for the graph dataset.What was easyThe original paper is well-structured. The code implementation is well-organized and with clear instructions on how to get started. This was helpful to understand the paper's work and begin experimenting with their proposed methods.What was difficultWe found it difficult to extrapolate some of the authors' proposed techniques to datasets other than those used by them.  Also, we were not able to reproduce the results for one of the experiments. We couldn't find the exact reason for it by running explorative experiments due to time and resource constraints.Communication with original authorsWe reached out to the authors once about our queries regarding one experimental setup and to understand the assumptions and contexts of some sub-claims in the paper. We received a prompt response which satisfied most of our questions.. Title: Reproducibility Study of \"Label-Free Explainability for Unsupervised Models\". Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0117", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: CRYPTOGRAPHICALLY SECURE FINANCIAL INSTRUMENTS, Integrated Combustion Reactor And Methods Of Conducting Simultaneous Endothermic and Exothermic Reactions? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0118", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: MiliPoint: A Point Cloud Dataset for mmWave Radar. Options: Reinforcement Learning, Applications, Probabilistic Methods.", "answer": "Applications"}
{"qid": "hard-0119", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers.By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language taskand image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be observed across many models trained on diverse datasets.Based on our findings, we point out an explicit strategy to identify, without supervision, the layers that maximize semantic content: representations at intermediate layers corresponding to a relative minimum of the ID profile are more suitable for downstream learning tasks.. Title: AndroidInTheWild: A Large-Scale Dataset For Android Device Control. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0120", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Smooth Flipping Probability for Differential Private Sign Random Projection Methods. Options: Reinforcement Learning, Deep Learning, Social Aspects.", "answer": "Social Aspects"}
{"qid": "hard-0121", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: TECHNOLOGIES FOR RACK ARCHITECTURE, INFORMATION PROCESSING DEVICE, INFORMATION MANAGEMENT METHOD, AND INFORMATION PROCESSING SYSTEM? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0122", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: APPARATUS AND METHOD FOR COMMUNICATIONS MANAGEMENT, NEXT GENERATION INTELLIGENT MESH NETWORK WITH FRONTHAUL AND BACKHAUL SERVICES? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0123", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Efficient Training of Energy-Based Models Using Jarzynski Equality. Options: Theory, Deep Learning, Probabilistic Methods.", "answer": "Deep Learning"}
{"qid": "hard-0124", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently exploring the environment, through a new design of policy distributions. The experimental results demonstrate that KIAN outperforms alternative methods incorporating external knowledge policies and achieves efficient and flexible learning. Our implementation is available at https://github.com/Pascalson/KGRL.git .. Title: Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0125", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This can be caused by the synthetic samples not perfectly representing class categories in real data and uniform sampling not necessarily providing useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples for regularizing feature extractors instead of training classifiers. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of naive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines by up to 7 percentage points on test accuracy.. Title: Towards Personalized Federated Learning via Heterogeneous Model Reassembly. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0126", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: RECEIVING POSITIONING SIGNALS AT DIFFERENT FREQUENCIES, LOUDSPEAKER? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0127", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The tremendous success of large models trained on extensive datasets demonstrates that scale is a key ingredient in achieving superior results. Therefore, the reflection on the rationality of designing knowledge distillation (KD) approaches for limited-capacity architectures solely based on small-scale datasets is now deemed imperative. In this paper, we identify the small data pitfall that presents in previous KD methods, which results in the underestimation of the power of vanilla KD framework on large-scale datasets such as ImageNet-1K. Specifically, we show that employing stronger data augmentation techniques and using larger datasets can directly decrease the gap between vanilla KD and other meticulously designed KD variants. This highlights the necessity of designing and evaluating KD approaches in the context of practical scenarios, casting off the limitations of small-scale datasets. Our investigation of the vanilla KD and its variants in more complex schemes, including stronger training strategies and different model capacities, demonstrates that vanilla KD is elegantly simple but astonishingly effective in large-scale scenarios. Without bells and whistles, we obtain state-of-the-art ResNet-50, ViT-S, and ConvNeXtV2-T models for ImageNet, which achieve 83.1%, 84.3%, and 85.0% top-1 accuracy, respectively. PyTorch code and checkpoints can be found at https://github.com/Hao840/vanillaKD.. Title: Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0128", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: KINETIC AND DIMENSIONAL OPTIMIZATION FOR A TENDON-DRIVEN GRIPPER, HYDRODYNAMIC COMPRESSION OR CUTTING TOOL? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0129", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: REAL-TIME SURGICAL REFERENCE INDICIUM APPARATUS AND METHODS FOR ASTIGMATISM CORRECTION, JAK1 SELECTIVE INHIBITOR AND USES THEREOF? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0130", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Neural Temporal Point Processes (TPPs)  are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real world applications, the event data typically comes in a streaming manner, where the distribution of the patterns may shift over time. Under the privacy and memory constraints commonly seen in real scenarios, how to continuously monitor a TPP to learn the streaming event sequence is an important yet under-investigated problem. In this work, we approach this problem by adopting Continual Learning (CL), which aims to enable a model to continuously learn a sequence of tasks without catastrophic forgetting. While CL for event sequence is less well studied, we present a simple yet effective framework, PromptTPP, by integrating the base TPP with a continuous-time retrieval prompt pool. In our proposed framework, prompts are small learnable parameters, maintained in a memory space and jointly optimized with the base TPP so that the model is properly instructed to learn event streams arriving sequentially without buffering past examples or task-specific attributes. We formalize a novel and realistic experimental setup for modeling event streams, where PromptTPP consistently sets state-of-the-art performance across two real user behavior datasets.. Title: Prompt-augmented Temporal Point Process for Streaming Event Sequence. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0131", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback. Options: Applications, Probabilistic Methods, Reinforcement Learning.", "answer": "Reinforcement Learning"}
{"qid": "hard-0132", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Object Retention System, TEM SAMPLE PREPARATION? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0133", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: NATURAL GAS FUEL SYSTEM FOR AN INTERNAL COMBUSTION ENGINE, METHODS AND SYSTEMS FOR BOOST CONTROL? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0134", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: EXTERNAL FIXING DEVICE FOR TREATING A FRACTURE OF CALCANEAL, PROSTHETIC HUMERAL HEAD COMPONENT? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0135", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\\mathcal{N}(0,\\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\\log(T/\\sigma))$. For the non-noisy version of the same class (i.e., $\\sigma=0$), we prove a lower bound of $\\Omega (wT)$ for the sample complexity.   Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\\sigma$, this gap still holds even for numerically negligible values of $\\sigma$.. Title: On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0136", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: LASER MICROSCOPE APPARATUS INCLUDING PHOTODETECTOR HAVING A PLURALITY OF DETECTION ELEMENTS, CONTROLLABLE COUPLING ASSEMBLY HAVING FORWARD AND REVERSE BACKLASH? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0137", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Deep learning models have been widely used to assist doctors with clinical decision-making. However, these models often encounter a significant performance drop when applied to data that differs from the distribution they were trained on. This challenge is known as the domain shift problem. Existing domain generalization algorithms attempt to address this problem by assuming the availability of domain IDs and training a single model to handle all domains. However, in healthcare settings, patients can be classified into numerous latent domains, where the actual domain categorizations are unknown. Furthermore, each patient domain exhibits distinct clinical characteristics, making it sub-optimal to train a single model for all domains. To overcome these limitations, we propose SLGD, a self-learning framework that iteratively discovers decoupled domains and trains personalized classifiers for each decoupled domain. We evaluate the generalizability of SLGD across spatial and temporal data distribution shifts on two real-world public EHR datasets: eICU and MIMIC-IV. Our results show that SLGD achieves up to 11% improvement in the AUPRC score over the best baseline.. Title: An Iterative Self-Learning Framework for Medical Domain Generalization. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0138", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through complex environments based on natural language instructions. In contrast to conventional approaches, which primarily focus on the spatial domain exploration, we propose a paradigm shift toward the Fourier domain. This alternative perspective aims to enhance visual-textual matching, ultimately improving the agent's ability to understand and execute navigation tasks based on the given instructions. In this study, we first explore the significance of high-frequency information in VLN and provide evidence that it is instrumental in bolstering visual-textual matching processes. Building upon this insight, we further propose a sophisticated and versatile Frequency-enhanced Data Augmentation (FDA) technique to improve the VLN model's capability of capturing critical high-frequency information. Specifically, this approach requires the agent to navigate in environments where only a subset of high-frequency visual information corresponds with the provided textual instructions, ultimately fostering the agent's ability to selectively discern and capture pertinent high-frequency features according to the given instructions. Promising results on R2R, RxR, CVDN and REVERIE demonstrate that our FDA can be readily integrated with existing VLN approaches, improving performance without adding extra parameters, and keeping models simple and efficient. The code is available at https://github.com/hekj/FDA.. Title: Does Invariant Graph Learning via Environment Augmentation Learn Invariance?. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0139", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SOLDER-CONTAINING SEMICONDUCTOR DEVICE, MOUNTED SOLDER-CONTAINING SEMICONDUCTOR DEVICE, PRODUCING METHOD AND MOUNTING METHOD OF SOLDER-CONTAINING SEMICONDUCTOR DEVICE, HIGH-STRENGTH PC STEEL WIRE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0140", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget. Options: Deep Learning, Applications, Miscellaneous Aspects of Machine Learning.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0141", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Provably Safe Reinforcement Learning with Step-wise Violation Constraints. Options: Social Aspects, Reinforcement Learning, Applications.", "answer": "Reinforcement Learning"}
{"qid": "hard-0142", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Synchronized Side-by-Side Display of Live Video and Corresponding Virtual Environment Images, Apparatus, Article of Manufacture, and Methods for In-Store Preview of an Online Jewelry Item? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0143", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Estimating Noise Correlations Across Continuous Conditions With Wishart Processes. Options: Applications, Deep Learning, Probabilistic Methods.", "answer": "Applications"}
{"qid": "hard-0144", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Vanilla spiking neurons in Spiking Neural Networks (SNNs) use charge-fire-reset neuronal dynamics, which can only be simulated serially and can hardly learn long-time dependencies. We find that when removing reset, the neuronal dynamics can be reformulated in a non-iterative form and parallelized. By rewriting neuronal dynamics without reset to a general formulation, we propose the Parallel Spiking Neuron (PSN), which generates hidden states that are independent of their predecessors, resulting in parallelizable neuronal dynamics and extremely high simulation speed. The weights of inputs in the PSN are fully connected, which maximizes the utilization of temporal information. To avoid the use of future inputs for step-by-step inference, the weights of the PSN can be masked, resulting in the masked PSN. By sharing weights across time-steps based on the masked PSN, the sliding PSN is proposed to handle sequences of varying lengths. We evaluate the PSN family on simulation speed and temporal/static data classification, and the results show the overwhelming advantage of the PSN family in efficiency and accuracy. To the best of our knowledge, this is the first study about parallelizing spiking neurons and can be a cornerstone for the spiking deep learning research. Our codes are available at https://github.com/fangwei123456/Parallel-Spiking-Neuron.. Title: Sheaf Hypergraph Networks. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0145", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer. Options: Theory, Applications, Optimization.", "answer": "Applications"}
{"qid": "hard-0146", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Neural networks with random weights appear in a variety of machine learning applications, most prominently as the initialization of many deep learning algorithms and as a computationally cheap alternative to fully learned neural networks. In the present article, we enhance the theoretical understanding of random neural networks by addressing the following data separation problem: under what conditions can a random neural network make two classes $\\mathcal{X}^-, \\mathcal{X}^+ \\subset \\mathbb{R}^d$ (with positive distance) linearly separable? We show that a sufficiently large two-layer ReLU-network with standard Gaussian weights and uniformly distributed biases can solve this problem with high probability. Crucially, the number of required neurons is explicitly linked to geometric properties of the underlying sets $\\mathcal{X}^-, \\mathcal{X}^+$ and their mutual arrangement. This instance-specific viewpoint allows us to overcome the usual curse of dimensionality (exponential width of the layers) in non-pathological situations where the data carries low-complexity structure. We quantify the relevant structure of the data in terms of a novel notion of mutual complexity (based on a localized version of Gaussian mean width), which leads to sound and informative separation guarantees. We connect our result with related lines of work on approximation, memorization, and generalization.. Title: The Separation Capacity of Random Neural Networks. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0147", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: DEVICE PAIRING VIA TRUSTED INTERMEDIARY, PERMANENT MAGNETIC CHUCK FOR OLED MASK CHUCKING? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0148", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with \"wrong\" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and certain implicit biases in common data collection practices. As we prove in this work, pessimism endows the agent with a survival instinct, i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival policies. Formally, given a reward class -- which may not even contain the true reward -- we identify conditions on the training data distribution that enable offline RL to learn a near-optimal and safe policy from any reward within the class. We argue that the survival instinct should be taken into account when interpreting results from existing offline RL benchmarks and when creating future ones. Our empirical and theoretical results suggest a new paradigm for offline RL, whereby an agent is \"nudged\" to learn a desirable behavior with imperfect reward but purposely biased data coverage. Please visit our website https://survival-instinct.github.io for accompanied code and videos.. Title: Context-guided Embedding Adaptation for Effective Topic Modeling in Low-Resource Regimes. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0149", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: INCISION PORTION STRUCTURE, HAVING REINFORCEMENT PORTIONS TO BE FORMED ON CORNEA, FOR REMOVING LENTICULE INCISED ACCORDING TO EYE SIGHT CORRECTION SURGERY, CONJUGATES? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0150", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation. Options: Miscellaneous Aspects of Machine Learning, Reinforcement Learning, Applications.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0151", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Asymptotically Optimal Quantile Pure Exploration for Infinite-Armed Bandits. Options: Theory, Applications, Deep Learning.", "answer": "Theory"}
{"qid": "hard-0152", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Diffusion models have achieved remarkable success in diverse domains such as image synthesis, super-resolution, and 3D molecule generation. Surprisingly, the application of diffusion models in graph learning has garnered little attention. In this paper, we aim to bridge this gap by exploring the use of diffusion models for unsupervised graph representation learning. Our investigation commences with the identification of anisotropic structures within graphs and the recognition of a crucial limitation in the vanilla forward diffusion process when dealing with these anisotropic structures. The original forward diffusion process continually adds  isotropic Gaussian noise to the data, which may excessively dilute anisotropic signals, leading to rapid signal-to-noise conversion. This rapid conversion poses challenges for training denoising neural networks and obstructs the acquisition of semantically meaningful representations during the reverse process. To overcome this challenge, we introduce a novel class of models termed {\\it directional diffusion models}.  These models adopt data-dependent, anisotropic, and directional noises in the forward diffusion process. In order to assess the effectiveness of our proposed models, we conduct extensive experiments on 12 publicly available datasets, with a particular focus on two distinct graph representation learning tasks. The experimental results unequivocally establish the superiority of our models over state-of-the-art baselines, underscoring their effectiveness in capturing meaningful graph representations. Our research not only sheds light on the intricacies of the forward process in diffusion models but also underscores the vast potential of these models in addressing a wide spectrum of graph-related tasks. Our code is available at \\url{https://github.com/statsle/DDM}.. Title: Directional diffusion models for graph representation learning. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0153", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: VEHICLE CONTROL DEVICE, VEHICLE CONTROL METHOD, AND VEHICLE CONTROL PROGRAM, INTEGRATED BATTERY CONTROL SYSTEM? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0154", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The causal revolution has stimulated interest in understanding complex relationships in various fields. Most of the existing methods aim to discover causal relationships among all variables within a complex large-scale graph. However, in practice, only a small subset of variables in the graph are relevant to the outcomes of interest. Consequently, causal estimation with the full causal graph---particularly given limited data---could lead to numerous falsely discovered, spurious variables that exhibit high correlation with, but exert no causal impact on, the target outcome. In this paper, we propose learning a class of necessary and sufficient causal graphs (NSCG) that exclusively comprises causally relevant variables for an outcome of interest, which we term causal features. The key idea is to employ probabilities of causation to systematically evaluate the importance of features in the causal graph, allowing us to identify a subgraph relevant to the outcome of interest. To learn NSCG from data, we develop a necessary and sufficient causal structural learning (NSCSL) algorithm, by establishing theoretical properties and relationships between probabilities of causation and natural causal effects of features. Across empirical studies of simulated and real data, we demonstrate that NSCSL outperforms existing algorithms and can reveal crucial yeast genes for target heritable traits of interest.. Title: Quasi-Monte Carlo Graph Random Features. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0155", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The success of meta-learning on out-of-distribution (OOD) tasks in the wild has proved to be hit-and-miss.To safeguard the generalization capability of the meta-learned prior knowledge to OOD tasks, in particularly safety-critical applications, necessitates detection of an OOD task followed by adaptation of the task towards the prior. Nonetheless, the reliability of estimated uncertainty on OOD tasks by existing Bayesian meta-learning methods is restricted by incomplete coverage of the feature distribution shift and insufficient expressiveness of the meta-learned prior. Besides, they struggle to adapt an OOD task, running parallel to the line of cross-domain task adaptation solutions which are vulnerable to overfitting.To this end, we build a single coherent framework that supports both detection and adaptation of OOD tasks, while remaining compatible with off-the-shelf meta-learning backbones. The proposed Energy-Based Meta-Learning (EBML) framework learns to characterize any arbitrary meta-training task distribution with the composition of two expressive neural-network-based energy functions. We deploy the sum of the two energy functions, being proportional to the joint distribution of a task, as a reliable score for detecting OOD tasks; during meta-testing, we adapt the OOD task to in-distribution tasks by energy minimization.Experiments on four regression and classification datasets  demonstrate the effectiveness of our proposal.. Title: Data Quality in Imitation Learning. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0156", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection. Options: Applications, Optimization, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0157", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Alignment with human representations supports robust few-shot learning. Options: Social Aspects, Theory, Applications.", "answer": "Applications"}
{"qid": "hard-0158", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: TILT MINIMIZATION THROUGH INTENSITY CONTROL OF LIGHT SOURCE, MICRO-ENVIRONMENT MODULE ARCHITECTURE FOR INFANT CARE DEVICES? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0159", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation. Options: Applications, Social Aspects, Reinforcement Learning.", "answer": "Applications"}
{"qid": "hard-0160", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: VACUUM CLEANER, FLEXIBLE FOLDING BATTERY COVER? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0161", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Audio Announcement Prioritization System, NETWORK ACCESS TECHNOLOGY INDICATION? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0162", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Packaged Semiconductor Devices and Methods of Packaging Semiconductor Devices, METHOD AND SYSTEM FOR MANAGING WIREDLY AND WIRELESSLY CHARGING RECHARGEABLE DEVICES AS WELL AS WIRELESSLY MANAGING RECHARGEABLE BATTERIES THEREOF USING A SMART ADAPTOR SUBSYSTEM? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0163", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PERSPIRATION-ABSORBENT AND QUICK-DRYING FABRIC AND METHOD FOR MANUFACTURING SAME, FIBER REINFORCED TISSUE COMPOSITES? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0164", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: INTERCOOLED GAS TURBINE OPTIMIZATION, ENGINE REMOTE START CONTROL METHOD AND SYSTEM? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0165", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.. Title: Unconstrained Dynamic Regret via Sparse Coding. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0166", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and the utilization of the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs competitively on two distinct representative LM tasks --- discrete-prompt generation and text summarization.. Title: Preference-grounded Token-level Guidance for Language Model Fine-tuning. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0167", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that scissorhands reduces the inference memory usage of the KV cache by up to 5$\\times$ without compromising model quality. We further demonstrate that scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20$\\times$ compression.. Title: [Re] $\\mathcal{G}$-Mixup: Graph Data Augmentation for Graph Classification. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0168", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: OSCILLATOR CONTROLLED RANDOM SAMPLING METHOD AND CIRCUIT, NONVOLATILE SEMICONDUCTOR MEMORY DEVICE AND METHOD OF MANUFACTURING THE SAME? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0169", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Contextual decision-making problems have witnessed extensive applications in various fields such as online content recommendation, personalized healthcare, and autonomous vehicles, where a core practical challenge is to select a suitable surrogate model for capturing unknown complicated reward functions. It is often the case that both high approximation accuracy and explicit uncertainty quantification are desired. In this work, we propose a neural network-accompanied Gaussian process (NN-AGP) model, which leverages neural networks to approximate the unknown and potentially complicated reward function regarding the contextual variable, and maintains a Gaussian process surrogate model with respect to the decision variable. Our model is shown to outperform existing approaches by offering better approximation accuracy thanks to the use of neural networks and possessing explicit uncertainty quantification from the Gaussian process. We also analyze the maximum information gain of the NN-AGP model and prove regret bounds for the corresponding algorithms. Moreover, we conduct experiments on both synthetic and practical problems, illustrating the effectiveness of our approach.. Title: SmoothHess: ReLU Network Feature Interactions via Stein's Lemma. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0170", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Tetradentate Platinum (II) Complexes Cyclometalated With Functionalized Phenyl Carbene Ligands And Their Analogues, SEMICONDUCTOR DEVICE INCLUDING FIN SHAPED STRUCTURE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0171", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Long-tail learning has received significant attention in recent years due to the challenge it poses with extremely imbalanced datasets. In these datasets, only a few classes (known as the head classes) have an adequate number of training samples, while the rest of the classes (known as the tail classes) are infrequent in the training data. Re-sampling is a classical and widely used approach for addressing class imbalance issues. Unfortunately, recent studies claim that re-sampling brings negligible performance improvements in modern long-tail learning tasks. This paper aims to investigate this phenomenon systematically. Our research shows that re-sampling can considerably improve generalization when the training images do not contain semantically irrelevant contexts. In other scenarios, however, it can learn unexpected spurious correlations between irrelevant contexts and target labels. We design experiments on two homogeneous datasets, one containing irrelevant context and the other not, to confirm our findings. To prevent the learning of spurious correlations, we propose a new context shift augmentation module that generates diverse training images for the tail class by maintaining a context bank extracted from the head-class images. Experiments demonstrate that our proposed module can boost the generalization and outperform other approaches, including class-balanced re-sampling, decoupled classifier re-training, and data augmentation methods. The source code is available at https://www.lamda.nju.edu.cn/code_CSA.ashx.. Title: Likelihood Ratio Confidence Sets for Sequential Decision Making. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0172", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities. Options: Social Aspects, Probabilistic Methods, Applications.", "answer": "Social Aspects"}
{"qid": "hard-0173", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The accurate predictions and principled uncertainty measures provided by GP regression incur $O(n^3)$ cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size $n$ increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as $n \\rightarrow \\infty$, uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibrated uncertainty measures and accurate predictions at remarkably low computational cost. We exhibit a very simple GPnn regression algorithm with stand-out performance compared to other state-of-the-art GP approximations as measured on large UCI datasets. It operates at a small fraction of those other methods' training costs, for example on a basic laptop taking about 30 seconds to train on a dataset of size $n = 1.6 \\times 10^6$.. Title: Learning from Rich Semantics and Coarse Locations for Long-tailed Object Detection. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0174", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: TRANSMITTER AND RECEIVER FOR A WIRELESS COMMUNICATION SYSTEM, DIALYSIS MACHINE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0175", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: VENTILATION FAN AND VENTILATION SYSTEM, CASE FOR AN ELECTRONIC DEVICE AND MANUFACTURING METHODS FOR MAKING A CASE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0176", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Federated Learning via Meta-Variational Dropout. Options: Social Aspects, Miscellaneous Aspects of Machine Learning, Deep Learning.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0177", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: METHOD OF SHARING A UE RECEIVER BETWEEN D2D AND CELLULAR OPERATIONS BASED ON ACTIVITY, Concurrent Multi-Loudspeaker Calibration? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0178", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: DISPOSABLE CUP INSERT FOR PAD PRINTING AND DECORATING, PRINTING APPARATUS FOR USE WITH SHEET PROCESSING APPARATUS, CONTROL METHOD AND STORAGE MEDIUM THEREFOR, AND PRINTING SYSTEM? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0179", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: ANALYTICAL TEST STRIP WITH INTEGRATED BATTERY, SENSOR ARRAY FOR DETECTING CONTROL GESTURES ON VEHICLES? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0180", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. One increasingly common way to annotate documents cheaply at scale is through large language models (LLMs). However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. We present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical properties—like asymptotic unbiasedness and proper uncertainty quantification—which are fundamental to CSS research. We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90\\%. To address this, we build on debiased machine learning to propose the design-based supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.. Title: Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0181", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Fast Online Changepoint Detection via Functional Pruning CUSUM Statistics. Options: Miscellaneous Aspects of Machine Learning, Applications, Social Aspects.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0182", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SEAT LOCK DEVICE, LOWER GUIDE FRAME OF A VERTICAL SUSPENSION SYSTEM, SLIDING SEAT GUIDE OF A VEHICLE SEAT, VERTICAL SUSPENSION SYSTEM FOR A VEHICLE SEAT, AND VEHICLE SEAT? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0183", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Labeling Neural Representations with Inverse Recognition. Options: Reinforcement Learning, Applications, Social Aspects.", "answer": "Social Aspects"}
{"qid": "hard-0184", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Whenever a clinician reflects on the efficacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient's health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to retrospectively analyze sequential decision making processes, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A* algorithm that, under a natural form of Lipschitz continuity of the environment’s dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks.. Title: Finding Counterfactually Optimal Action Sequences in Continuous State Spaces. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0185", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose method to estimate the parameters of statistical models from incomplete data. We validate VGI on a set of synthetic and real-world estimation tasks, estimating important machine learning models such as variational autoencoders and normalising flows from incomplete data. The proposed method, whilst general-purpose, achieves competitive or better performance than existing model-specific estimation methods.. Title: Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0186", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Connection Ring Arrangement for an Electrical Machine, LAUNDRY PRODUCT DOSING DEVICE? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0187", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. Options: Applications, Probabilistic Methods, Deep Learning.", "answer": "Applications"}
{"qid": "hard-0188", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: This work presents a graph neural network (GNN) framework for solving the maximum independent set (MIS) problem, inspired by dynamic programming (DP). Specifically, given a graph, we propose a DP-like recursive algorithm based on GNNs that firstly constructs two smaller sub-graphs, predicts the one with the larger MIS, and then uses it in the next recursive call. To train our algorithm, we require annotated comparisons of different graphs concerning their MIS size. Annotating the comparisons with the output of our algorithm leads to a self-training process that results in more accurate self-annotation of the comparisons and vice versa. We provide numerical evidence showing the superiority of our method vs prior methods in multiple synthetic and real-world datasets.. Title: Maximum Independent Set: Self-Training through Dynamic Programming. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0189", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEM AND METHOD FOR THREE DIMENSIONAL IMAGING, AIS Data Transmission? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0190", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Existing methods for adapting Neural Radiance Fields (NeRFs) to scene changes require extensive data capture and model retraining, which is both time-consuming and labor-intensive. In this paper, we tackle the challenge of efficiently adapting NeRFs to real-world scene changes over time using a few new images while retaining the memory of unaltered areas, focusing on the continual learning aspect of NeRFs. To this end, we propose CL-NeRF, which consists of two key components: a lightweight expert adaptor for adapting to new changes and evolving scene representations and a conflict-aware knowledge distillation learning objective for memorizing unchanged parts. We also present a new benchmark for evaluating Continual Learning of NeRFs with comprehensive metrics. Our extensive experiments demonstrate that CL-NeRF can synthesize high-quality novel views of both changed and unchanged regions with  high training efficiency, surpassing existing methods in terms of reducing forgetting and adapting to changes. Code and benchmark will be made available.. Title: AndroidInTheWild: A Large-Scale Dataset For Android Device Control. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0191", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: 3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes. Options: Applications, Theory, Deep Learning.", "answer": "Applications"}
{"qid": "hard-0192", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems’ outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall·E 2 , Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed forthis work, as well as the necessary tools to similarly evaluate additional TTI systems.. Title: Stable Bias: Evaluating Societal Representations in Diffusion Models. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0193", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: SceneScape: Text-Driven Consistent Scene Generation. Options: Reinforcement Learning, Applications, Deep Learning.", "answer": "Applications"}
{"qid": "hard-0194", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond. Options: Applications, Reinforcement Learning, Social Aspects.", "answer": "Social Aspects"}
{"qid": "hard-0195", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $O(n^3/s^2)$ which outperforms the best-known bound $O(n^3)$ by a factor of $s^2$, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our results illustrate that EL converges up to  $ 1.7\\times$ quicker than baseline DL algorithms and attains $2.2 $\\% higher accuracy for the same communication volume.. Title: A Logic for Expressing Log-Precision Transformers. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0196", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning. Options: Social Aspects, Applications, Probabilistic Methods.", "answer": "Applications"}
{"qid": "hard-0197", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: STORAGE CONTAINER WITH SPOUT, SPARK PLUG STORAGE DEVICE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0198", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Better Correlation and Robustness: A Distribution-Balanced Self-Supervised Learning Framework for Automatic Dialogue Evaluation. Options: Social Aspects, Deep Learning, Optimization.", "answer": "Deep Learning"}
{"qid": "hard-0199", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: COMPOSITIONS AND METHODS FOR TREATING OCULAR DISEASES, Systems and Methods for Performing Layer One Link Aggregation Over Wireless Links? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0200", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning.  Their performance is well measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm.  Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a  weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by slow mixing of ULA. We illustrate these results with numerical experiments on Gaussian mixture distributions as well as the MNIST and CIFAR-10 datasets. We show that the proposed approach outperforms  methods based on the contrastive divergence algorithm in all the considered situations.. Title: POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0201", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Expressivity-Preserving GNN Simulation. Options: Social Aspects, Deep Learning, Probabilistic Methods.", "answer": "Deep Learning"}
{"qid": "hard-0202", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PROJECTION DISPLAY APPARATUS, IMAGE CAPTURE APPARATUS AND METHOD FOR CONTROLLING THE SAME? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0203", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: IMAGE GENERATION APPARATUS, DECORATIVE EYEGLASSES LOCATOR? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0204", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Mnemosyne: Learning to Train Transformers with Transformers. Options: Applications, Deep Learning, Miscellaneous Aspects of Machine Learning.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0205", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEMS AND METHODS FOR IMPROVING GEAR SHIFTS, ORTHOPEDIC KNEEPAD? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0206", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This requirement overly simplifies the natural subjectivity present in many tasks, and obscures the inherent diversity in human perceptions and opinions about many content items. Preserving the variance in content and diversity in human perceptions in datasets is often quite expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is socio-culturally situated in this context. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographics information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different aggregation strategies. The DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of safety for conversational AI. We further describe a set of metrics that show how rater diversity influences safety perception across different geographic regions, ethnicity groups, age groups, and genders. The goal of the DICES dataset is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.. Title: [Re] CrossWalk: Fairness-enhanced Node Representation Learning. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0207", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We reveal a one-class homophily phenomenon, which is one prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction.In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity-- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations.  We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be biased by non-homophily edges(i.e., edges connecting normal and abnormal nodes). Thus, TAM is instead optimized on truncated graphs where non-homophily edges are removed iteratively to mitigate this bias. The learned representations result in significantly stronger local affinity for normal nodes than abnormal nodes. Extensive empirical results on 10 real-world GAD datasets show that TAM substantially outperforms seven competing models, achieving over 10% increase in AUROC/AUPRC compared to the best contenders on challenging datasets. Our code is available at https://github.com/mala-lab/TAM-master/.. Title: Customizable Image Synthesis with Multiple Subjects. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0208", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes?In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.. Title: Beyond Average Return in Markov Decision Processes. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0209", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PACKAGING COMPRISING TEAR-OFF LID AND DOSING SPOON, PROCESS FOR FORMING AN INSULATED CONTAINER HAVING ARTWORK? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0210", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation. This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data. However, it becomes more  challenging when faced with a scarcity of paired data and labelling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like “tasty”, whereas movie reviews commonly contain words such as “thrilling” for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly over domains. In this work, we tackle the domain-varying dependence between the content and the style variables inherent in the counterfactual generation task. We provide identification guarantees for such latent-variable models by leveraging the relative sparsity of the influences from different latent variables. Our theoretical insights enable the development of a doMain AdapTive counTerfactual gEneration model, called (MATTE). Our theoretically grounded framework achieves state-of-the-art performance in unsupervised style transfer tasks, where neither paired data nor style labels are utilized, across four large-scale datasets.. Title: Counterfactual Generation with Identifiability Guarantees. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0211", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The causal revolution has stimulated interest in understanding complex relationships in various fields. Most of the existing methods aim to discover causal relationships among all variables within a complex large-scale graph. However, in practice, only a small subset of variables in the graph are relevant to the outcomes of interest. Consequently, causal estimation with the full causal graph---particularly given limited data---could lead to numerous falsely discovered, spurious variables that exhibit high correlation with, but exert no causal impact on, the target outcome. In this paper, we propose learning a class of necessary and sufficient causal graphs (NSCG) that exclusively comprises causally relevant variables for an outcome of interest, which we term causal features. The key idea is to employ probabilities of causation to systematically evaluate the importance of features in the causal graph, allowing us to identify a subgraph relevant to the outcome of interest. To learn NSCG from data, we develop a necessary and sufficient causal structural learning (NSCSL) algorithm, by establishing theoretical properties and relationships between probabilities of causation and natural causal effects of features. Across empirical studies of simulated and real data, we demonstrate that NSCSL outperforms existing algorithms and can reveal crucial yeast genes for target heritable traits of interest.. Title: On Learning Necessary and Sufficient Causal Graphs. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0212", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters.   Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input.  Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup.  Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input.  We use our framework in order to prove that, under mild assumptions, when the pretraining distribution is a mixture of latent tasks (a model often considered for natural language pretraining), these tasks can be efficiently learned via in-context learning, even though the model's weights are unchanged and the input significantly diverges from the pretraining distribution.  Our theoretical analysis reveals that in this setting, in-context learning is more about identifying the task than about learning it, a result which is in line with a series of recent empirical findings.   We hope that the in-context learnability framework presented in this paper will facilitate future progress towards a deeper understanding of this important new learning paradigm.. Title: Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0213", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SMART PUMP, BIOCHEMICAL REACTIONS SYSTEM? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0214", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation. Options: Social Aspects, Applications, Probabilistic Methods.", "answer": "Applications"}
{"qid": "hard-0215", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks. Our project page: https://dreamsim-nights.github.io/. Title: Linear Time Algorithms for k-means with Multi-Swap Local Search. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0216", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Zeroth-Order Methods for Nondifferentiable, Nonconvex, and Hierarchical Federated Optimization. Options: Social Aspects, Optimization, Probabilistic Methods.", "answer": "Optimization"}
{"qid": "hard-0217", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Concentration analysis of multivariate elliptic diffusions. Options: Theory, Optimization, Probabilistic Methods.", "answer": "Theory"}
{"qid": "hard-0218", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: The Distortion of Binomial Voting Defies Expectation. Options: Theory, Probabilistic Methods, Deep Learning.", "answer": "Theory"}
{"qid": "hard-0219", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab. Options: Reinforcement Learning, Theory, Applications.", "answer": "Applications"}
{"qid": "hard-0220", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Synthesizing images with user-specified subjects has received growing attention due to its practical applications. Despite the recent success in single subject customization, existing algorithms suffer from high training cost and low success rate along with increased number of subjects. Towards controllable image synthesis with multiple subjects as the constraints, this work studies how to efficiently represent a particular subject as well as how to appropriately compose different subjects. We find that the text embedding regarding the subject token already serves as a simple yet effective representation that supports arbitrary combinations without any model tuning. Through learning a residual on top of the base embedding, we manage to robustly shift the raw subject to the customized subject given various text conditions. We then propose to employ layout, a very abstract and easy-to-obtain prior, as the spatial guidance for subject arrangement. By rectifying the activations in the cross-attention map, the layout appoints and separates the location of different subjects in the image, significantly alleviating the interference across them. Using cross-attention map as the intermediary, we could strengthen the signal of target subjects and weaken the signal of irrelevant subjects within a certain region, significantly alleviating the interference across subjects. Both qualitative and quantitative experimental results demonstrate our superiority over state-of-the-art alternatives under a variety of settings for multi-subject customization.. Title: Customizable Image Synthesis with Multiple Subjects. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0221", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Visual anomaly detection, the task of detecting abnormal characteristics in images, is challenging due to the rarity and unpredictability of anomalies. In order to reliably model the distribution of normality and detect anomalies, a few works have attempted to exploit the density estimation ability of normalizing flow (NF). However, previous NF-based methods have relied solely on the capability of NF and forcibly transformed the distribution of all features to a single distribution (e.g., unit normal distribution), when features can have different semantic information and thus follow different distributions. We claim that forcibly learning to transform such diverse distributions to a single distribution with a single network will cause the learning difficulty, limiting the capacity of a network to discriminate normal and abnormal data. As such, we propose to transform the distribution of features at each location of a given image to different distributions. In particular, we train NF to map normal data distribution to distributions with the same mean but different variances at each location of the given image. To enhance the discriminability, we also train NF to map abnormal data distribution to a distribution with a mean that is different from that of normal data, where abnormal data is synthesized with data augmentation. The experimental results outline the effectiveness of the proposed framework in improving the density modeling and thus anomaly detection performance.. Title: SwiFT: Swin 4D fMRI Transformer. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0222", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: IN-VEHICLE DEVICE, INFORMATION DISTRIBUTION SERVER, AND FACILITY INFORMATION DISPLAY METHOD, SAMPLE ANALYZER, SAMPLE ANALYZING METHOD, AND SAMPLE ANALYZING SYSTEM? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0223", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The Collaborative Research Cycle (CRC) is a National Institute of Standards and Technology (NIST) benchmarking program intended to strengthen understanding of tabular data deidentification technologies. Deidentification algorithms are vulnerable to the same bias and privacy issues that impact other data analytics and machine learning applications, and it can even amplify those issues by contaminating downstream applications. This paper summarizes four CRC contributions: theoretical work on the relationship between diverse populations and challenges for equitable deidentification; public benchmark data focused on diverse populations and challenging features; a comprehensive open source suite of evaluation metrology for deidentified datasets; and an archive of more than 450 deidentified data samples from a broad range of techniques. The initial set of evaluation results demonstrate the value of the CRC tools for investigations in this field.. Title: A Variational Perspective on High-Resolution ODEs. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0224", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: FLOCCULATION, SYSTEMS AND METHODS FOR IMPROVING GEAR SHIFTS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0225", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Proximity-Informed Calibration for Deep Neural Networks. Options: Applications, Probabilistic Methods, Social Aspects.", "answer": "Social Aspects"}
{"qid": "hard-0226", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis. Options: Reinforcement Learning, Probabilistic Methods, Social Aspects.", "answer": "Reinforcement Learning"}
{"qid": "hard-0227", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Into the LAION’s Den: Investigating Hate in Multimodal Datasets. Options: Social Aspects, Optimization, Applications.", "answer": "Social Aspects"}
{"qid": "hard-0228", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Fairly Recommending with Social Attributes: A Flexible and Controllable Optimization Approach. Options: Probabilistic Methods, Deep Learning, Social Aspects.", "answer": "Social Aspects"}
{"qid": "hard-0229", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional \"content\" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining ``texture'' variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based model, while also yielding competitive performance with VAE-based models in several distortion metrics. Furthermore, training the diffusion with  $\\mathcal{X}$-parameterization enables high-quality reconstructions in only a handful of decoding steps, greatly affecting the model's practicality. Our code is available at: https://github.com/buggyyang/CDC_compression. Title: NAS-X: Neural Adaptive Smoothing via Twisting. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0230", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEMS AND METHODS INVOLVING MULTIPLE TORQUE PATHS FOR GAS TURBINE ENGINES, HOMOGENEOUS CHARGE COMPRESSION IGNITION ENGINE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0231", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Meta-in-context learning in large language models. Options: Social Aspects, Miscellaneous Aspects of Machine Learning, Deep Learning.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0232", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PROVIDING CONTENT TO DEVICES IN A CLUSTER, SOCIALLY-INTERACTIVE CAUSE PLATFORM AND METHOD OF USE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0233", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. Options: Probabilistic Methods, Theory, Applications.", "answer": "Theory"}
{"qid": "hard-0234", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Despite the stunning ability to generate high-quality images by recent text-to-image models, current approaches often struggle to effectively compose objects with different attributes and relationships into a complex and coherent scene. We propose T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional text prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). We further propose several evaluation metrics specifically designed to evaluate compositional text-to-image generation and explore the potential and limitations of multimodal LLMs for evaluation. We introduce a new approach, Generative mOdel finetuning with Reward-driven Sample selection (GORS), to boost the compositional text-to-image generation abilities of pretrained text-to-image models. Extensive experiments and evaluations are conducted to benchmark previous methods on T2I-CompBench, and to validate the effectiveness of our proposed evaluation metrics and GORS approach. Project page is available at https://karine-h.github.io/T2I-CompBench/.. Title: T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0235", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SHIFTED LENS CAMERA FOR MOBILE COMPUTING DEVICES, DIFFUSER PIPE WITH VORTEX GENERATORS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0236", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: SOC: Semantic-Assisted  Object Cluster for Referring Video Object Segmentation. Options: Applications, Social Aspects, Optimization.", "answer": "Applications"}
{"qid": "hard-0237", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: This work presents a graph neural network (GNN) framework for solving the maximum independent set (MIS) problem, inspired by dynamic programming (DP). Specifically, given a graph, we propose a DP-like recursive algorithm based on GNNs that firstly constructs two smaller sub-graphs, predicts the one with the larger MIS, and then uses it in the next recursive call. To train our algorithm, we require annotated comparisons of different graphs concerning their MIS size. Annotating the comparisons with the output of our algorithm leads to a self-training process that results in more accurate self-annotation of the comparisons and vice versa. We provide numerical evidence showing the superiority of our method vs prior methods in multiple synthetic and real-world datasets.. Title: Maximum Independent Set: Self-Training through Dynamic Programming. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0238", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: TACTICAL DEPLOYABLE CABLES, SURGICAL TOOL FOR ROBOTIC SURGERY AND ROBOTIC SURGICAL ASSEMBLY? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0239", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness. Options: Theory, Social Aspects, Optimization.", "answer": "Theory"}
{"qid": "hard-0240", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SYSTEMS AND METHODS FOR INITIATING AND AUTHORIZING TRANSACTIONS USING A DETECTABLE DEVICE, PATTERN-DRIVEN DATA GENERATOR? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0241", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Designing efficient algorithms to compute a Nash Equilibrium (NE) in multiplayer games is still an open challenge. In this paper, we focus on computing an NE that optimizes a given objective function. For example, when there is a team of players independently playing against an adversary in a game (e.g., several groups in a forest trying to interdict illegal loggers in green security games), these team members may need to find an NE minimizing the adversary’s utility. Finding an optimal NE in multiplayer games can be formulated as a mixed-integer bilinear program by introducing auxiliary variables to represent bilinear terms, leading to a huge number of bilinear terms, making it hard to solve. To overcome this challenge, we first propose a general framework for this formulation based on a set of correlation plans. We then develop a novel algorithm called CRM based on this framework, which uses correlation plans with their relations to strictly reduce the feasible solution space after the convex relaxation of bilinear terms while minimizing the number of correlation plans to significantly reduce the number of bilinear terms. We show that our techniques can significantly reduce the time complexity and CRM can be several orders of magnitude faster than the state-of-the-art baseline.. Title: A Combinatorial Algorithm for Approximating the Optimal Transport in the Parallel and MPC Settings. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0242", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: Piano Learning System, Image Display Apparatus And Image Display Method? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0243", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference. Options: Social Aspects, Reinforcement Learning, Probabilistic Methods.", "answer": "Social Aspects"}
{"qid": "hard-0244", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Knowledge-based visual question answering (VQA) requires external knowledge to answer the question about an image. Early methods explicitly retrieve knowledge from external knowledge bases, which often introduce noisy information. Recently large language models like GPT-3 have shown encouraging performance as implicit knowledge source and revealed planning abilities. However, current large language models can not effectively understand image inputs, thus it remains an open problem to extract the image information and input to large language models. Prior works have used image captioning and object descriptions to represent the image. However, they may either drop the essential visual information to answer the question correctly or involve irrelevant objects to the task-of-interest. To address this problem, we propose to let large language models make an initial hypothesis according to their knowledge, then actively collect the visual evidence required to verify the hypothesis. In this way, the model can attend to the essential visual information in a task-oriented manner. We leverage several vision modules from the perspectives of spatial attention (i.e., Where to look) and attribute attention (i.e., What to look), which is similar to human cognition. The experiments show that our proposed method outperforms the baselines on open-ended knowledge-based VQA datasets and presents clear reasoning procedure with better interpretability.. Title: TOA: Task-oriented Active VQA. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0245", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: SEMICONDUCTOR DEVICE WITH STACKED TERMINALS, COMPOSITE CABLE AND COMPOSITE HARNESS? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0246", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) towards specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We evaluate our method across various tasks, including summarization, dialogue response generation, and chain-of-thought reasoning. Our experiments indicate a consistent improvement in the performance of LLMs such as ChatGPT, Codex, and InstructGPT on these supervised tasks with minimal labeled data. Remarkably, by utilizing merely 80 dialogues from the MultiWOZ dataset, our approach boosts ChatGPT's performance by a relative 41.4%, achieving or exceeding the performance of some fully supervised state-of-the-art models. Moreover, the instance-specific chain-of-thought prompt generated through our method enhances InstructGPT's reasoning accuracy, outperforming both generalized human-crafted prompts and those generated through automatic prompt engineering. The code and data are publicly available at https://github.com/Leezekun/Directional-Stimulus-Prompting.. Title: ANPL: Towards Natural Programming with Interactive Decomposition. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0247", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: GLASS FOR MAGNETIC RECORDING MEDIUM SUBSTRATE, GLASS SUBSTRATE FOR MAGNETIC RECORDING MEDIUM, AND THEIR USE, SEMICONDUCTOR DEVICE AND INFORMATION PROCESSING DEVICE? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0248", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Asymptotically Optimal Quantile Pure Exploration for Infinite-Armed Bandits. Options: Probabilistic Methods, Theory, Deep Learning.", "answer": "Theory"}
{"qid": "hard-0249", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Scope of Reproducibility — This paper analyses the reproducibility of the study Proto2Proto: Can you recognize the car, the way I do? The main contributions and claims of the study are: 1) Using Proto2Proto, a shallower student model is more faithful to the teacher in terms of interpretability than a baseline student model while also showing the same or better accuracy; 2) Global Explanation loss forces student prototypes to be close to teacher prototypes; 3) Patch‐Prototype Correspondence loss enforces the local representations of the student to be similar to those of the teacher; 4) The proposed evaluation metrics determine the faithfulness of the student to the teacher in terms of interpretability.Methodology — A public code repository was available for the paper, which provided a working but incomplete and minimally documented codebase. With some modifications we were able to carry out the experiments that were best supported by the codebase. We spent a total of 60 computational GPU hours on reproduction.Results — The results we were able to produce support claim 1, albeit weakly. Further results are in line with the paper, but we found them to go against claim 3. In addition, we carried out a theoretical analysis which provides support for claim 4. Finally, we were unable to carry out our intended experiment to verify claim 2. What was easy — The original paper was clearly structured and understandable. The experiments for which configurations were provided were simple to conduct.What was difficult — The public codebase contained minimal documentation. Moreover, the use of variable names did not correspond between the code and the paper. Furthermore, the codebase lacked elements vital to reproducing some experiments. Another significant constraint were the computational requirements needed to reproduce the original experiments. Finally, the code required to reproduce one of the visualizations was not provided.Communication with original authors — We contacted the authors to ask for trained model weights and missing hyperparameters for several experiments. We did not receive aresponse.. Title: Reproducibility study of 'Proto2Proto: Can you recognise the car, the way I do?'. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0250", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: INDICATION INFORMATION TRANSMISSION METHOD AND APPARATUS, CONTENT REPRODUCTION SYSTEM, CONTENT REPRODUCTION APPARATUS, PROGRAM, CONTENT REPRODUCTION METHOD, AND PROVIDING CONTENT SERVER? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0251", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation. Options: Deep Learning, Reinforcement Learning, Applications.", "answer": "Applications"}
{"qid": "hard-0252", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty quantification for time series data. Furthermore, our results indicate that the proposed method can consistently identify the autoregressive order for time series data and outperform existing methods in large-scale model compression. Our proposed method has important practical implications in fields such as finance, healthcare, and energy, where both accurate point estimates and prediction uncertainty quantification are of concern.. Title: Euler-Lagrange Analysis of Generative Adversarial Networks. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0253", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: 3D Color Mapping and Tuning in an Image Processing Pipeline, DRUG DELIVERY COMPOSITIONS AND METHODS TARGETING P-GLYCOPROTEIN? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0254", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We study robust adversarial training of two-layer neural networks as a bi-level optimization problem. In particular, for the inner loop that implements the adversarial attack during training using projected gradient descent (PGD), we propose maximizing a \\emph{lower bound} on the $0/1$-loss by reflecting a surrogate loss about the origin. This allows us to give a convergence guarantee for the inner-loop PGD attack. Furthermore, assuming the data is linearly separable, we provide precise iteration complexity results for end-to-end adversarial training, which holds for any width and initialization. We provide empirical evidence to support our theoretical results.. Title: Robustness Guarantees for Adversarially Trained Neural Networks. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0255", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: IMAGE FORMING APPARATUS AND CONTROL METHOD THEREOF, METHOD AND SYSTEM FOR DISTRIBUTED OPTIMAL CACHING OF CONTENT OVER A NETWORK? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0256", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: LIVE CENTER VISCOUS CLUTCH, TOPICAL ERYTHROPOIETIN FORMULATIONS AND METHODS FOR IMPROVING WOUND HEALING WITH AND COSMETIC USE OF THE FORMULATIONS? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0257", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: IMPROVED POLYOLEFIN COMPOSITION FOR VEHICLE NOISE VIBRATION AND HARSHNESS APPLICATIONS, Process for Cold Bonding Rubber on Metal Substrates? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0258", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Sampling weights of deep neural networks. Options: Reinforcement Learning, Social Aspects, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0259", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: DRUG DELIVERY DEVICE, Bispecific Molecules That Are Immunoreactive With Immune Effector Cells That Express An Activating Receptor And An Antigen Expressed By A Cell Infected By A Virus And Uses Thereof? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0260", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks,  their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We implement these recommendations on two state-of-the-art bisimulation-based algorithms, MICo and SimSR, and demonstrate performance gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at \\url{https://github.com/zanghyu/Offline_Bisimulation}.. Title: Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated  Sparse Network Training. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0261", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Self-supervised video pretraining yields robust and more human-aligned visual representations. Options: Theory, Deep Learning, Social Aspects.", "answer": "Deep Learning"}
{"qid": "hard-0262", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Large Language Models (LLMs) are progressively being utilized as machine learning services and interface tools for various applications. However, the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined. In this paper, we propose TrojLLM, an automatic and black-box framework to effectively generate universal and stealthy triggers. When these triggers are incorporated into the input data, the LLMs' outputs can be maliciously manipulated.   Moreover, the framework also supports embedding Trojans within discrete prompts, enhancing the overall effectiveness and precision of the triggers' attacks.  Specifically, we propose a trigger discovery algorithm for generating universal triggers for various inputs by querying victim LLM-based APIs using few-shot data samples. Furthermore, we introduce a novel progressive Trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. Our experiments and results demonstrate TrojLLM's capacity to effectively insert Trojans into text prompts in real-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining exceptional performance on clean test sets. Our work sheds light on the potential security risks in current models and offers a potential defensive approach. The source code of TrojLLM is available at https://github.com/UCF-ML-Research/TrojLLM.. Title: Passive learning of active causal strategies in agents and language models. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0263", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: RF TRANSCEIVER WITH DISTRIBUTED FILTERING TOPOLOGY, MODULAR SIGN SYSTEM WITH A WIRELESS BACKPLANE AND RELATED METHODS? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0264", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. Options: Social Aspects, Applications, Deep Learning.", "answer": "Applications"}
{"qid": "hard-0265", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: We propose a Bayesian encoder for metric learning. Rather than relying on neural amortization as done in prior works, we learn a distribution over the network weights with the Laplace Approximation. We first prove that the contrastive loss is a negative log-likelihood on the spherical space. We propose three methods that ensure a positive definite covariance matrix. Lastly, we present a novel decomposition of the Generalized Gauss-Newton approximation. Empirically, we show that our Laplacian Metric Learner (LAM) yields well-calibrated uncertainties, reliably detects out-of-distribution examples, and has state-of-the-art predictive performance.. Title: Bayesian Metric Learning for Uncertainty Quantification in Image Retrieval. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0266", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection. Options: Deep Learning, Applications, Theory.", "answer": "Deep Learning"}
{"qid": "hard-0267", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In this paper, we provide a theoretical analysis of the inductive biases in convolutional neural networks (CNNs). We start by examining the universality of CNNs, i.e., the ability to approximate any continuous functions.  We prove that a depth of $\\mathcal{O}(\\log d)$ suffices for deep CNNs to achieve this universality, where $d$ in the input dimension.  Additionally, we establish  that learning sparse functions with  CNNs requires only $\\widetilde{\\mathcal{O}}(\\log^2d)$ samples, indicating that deep CNNs can efficiently capture {\\em long-range} sparse correlations. These results are made possible through a novel combination of the multichanneling and downsampling when increasing the network depth. We also delve into the distinct roles  of weight sharing and locality in CNNs. To this end, we compare the performance of CNNs, locally-connected networks (LCNs), and fully-connected networks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs without weight sharing. On the one hand,  we  prove that LCNs require ${\\Omega}(d)$ samples while CNNs need only $\\widetilde{\\mathcal{O}}(\\log^2d)$ samples,  highlighting the  critical role of weight sharing. On the other hand, we prove that FCNs require $\\Omega(d^2)$ samples, whereas LCNs need only $\\widetilde{\\mathcal{O}}(d)$ samples,  underscoring the importance  of locality. These provable separations quantify the difference between the two biases, and the major observation behind our proof is that weight sharing and locality break different symmetries in the learning process.. Title: Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0268", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PROCESSING APPARATUS AND PROCESSING METHOD, Supporting Device for a Plurality of Components with Different Geometries? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0269", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments.In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the \"Chain of Thoughts\" mode for effective embodied planning.(ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control.Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering.Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.. Title: EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0270", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: The recently developed sparse network training methods, such as Lottery Ticket Hypothesis (LTH) and its variants, have shown impressive learning capacity by finding sparse sub-networks from a dense one. While these methods could largely sparsify deep networks, they generally focus more on realizing comparable accuracy to dense counterparts yet neglect network calibration. However, how to achieve calibrated network predictions lies at the core of improving model reliability, especially when it comes to addressing the overconfident issue and out-of-distribution cases. In this study, we propose a novel Distributionally Robust Optimization (DRO) framework to achieve an ensemble of lottery tickets towards calibrated network sparsification. Specifically, the proposed DRO ensemble aims to learn multiple diverse and complementary sparse sub-networks (tickets) with the guidance of uncertainty sets, which encourage tickets to gradually capture different data distributions from easy to hard and naturally complement each other. We theoretically justify the strong calibration performance by showing how the proposed robust training process guarantees to lower the confidence of incorrect predictions. Extensive experimental results on several benchmarks show that our proposed lottery ticket ensemble leads to a clear calibration improvement without sacrificing accuracy and burdening inference costs. Furthermore, experiments on OOD datasets demonstrate the robustness of our approach in the open-set environment.. Title: What Do Deep Saliency Models Learn about Visual Attention?. Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0271", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: NETWORK ACCESS TECHNOLOGY INDICATION, POLYMER FIBER SCAFFOLDS AND USES THEREOF? Return 'Yes' or 'No'.", "answer": "No"}
{"qid": "hard-0272", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Prediction and Control in Continual Reinforcement Learning. Options: Reinforcement Learning, Optimization, Probabilistic Methods.", "answer": "Reinforcement Learning"}
{"qid": "hard-0273", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: III-V FIN Generation By Lateral Growth On Silicon Sidewall, WINDING DEVICE AND WINDING METHOD? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0274", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Improving Graph Matching with Positional Reconstruction Encoder-Decoder Network. Options: Probabilistic Methods, Deep Learning, Applications.", "answer": "Deep Learning"}
{"qid": "hard-0275", "question_type": "12", "question": "Predict if this abstract-title pair, which is not present in the database, is from the same NeurIPS paper: Abstract: In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates to significantly reduce communication costs, while achieving the {\\em same} convergence rates as those of their algorithmic counterparts in the single-objective federated learning. Our extensive experiments also corroborate the efficacy of our proposed FMOO algorithms.. Title: Federated Multi-Objective Learning. Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0276", "question_type": "11", "question": "Predict if the following two patents, which are not present in the database, belong to the same cpc category: PRODUCTION OF BETA-PHELLANDRENE USING GENETICALLY ENGINEERED PHOTOSYNTHETIC MICROORGANISMS, NOVEL PROMOTER AND USE THEREOF? Return 'Yes' or 'No'.", "answer": "Yes"}
{"qid": "hard-0277", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Resilient Constrained Learning. Options: Deep Learning, Optimization, Probabilistic Methods.", "answer": "Optimization"}
{"qid": "hard-0278", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Toolbox for Multimodal Learn (scikit-multimodallearn). Options: Applications, Miscellaneous Aspects of Machine Learning, Social Aspects.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0279", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Perception Test: A Diagnostic Benchmark for Multimodal Video Models. Options: Applications, Reinforcement Learning, Theory.", "answer": "Applications"}
{"qid": "hard-0280", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: What Do Deep Saliency Models Learn about Visual Attention?. Options: Optimization, Deep Learning, Applications.", "answer": "Applications"}
{"qid": "hard-0281", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Local Convergence of Gradient Methods for Min-Max Games: Partial Curvature Generically Suffices. Options: Optimization, Reinforcement Learning, Social Aspects.", "answer": "Optimization"}
{"qid": "hard-0282", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control. Options: Applications, Reinforcement Learning, Probabilistic Methods.", "answer": "Reinforcement Learning"}
{"qid": "hard-0283", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Sheaf Hypergraph Networks. Options: Deep Learning, Optimization, Applications.", "answer": "Deep Learning"}
{"qid": "hard-0284", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms. Options: Optimization, Probabilistic Methods, Reinforcement Learning.", "answer": "Reinforcement Learning"}
{"qid": "hard-0285", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Private Federated Frequency Estimation: Adapting to the Hardness of the Instance. Options: Deep Learning, Optimization, Social Aspects.", "answer": "Optimization"}
{"qid": "hard-0286", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework. Options: Social Aspects, Theory, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0287", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers. Options: Applications, Theory, Social Aspects.", "answer": "Applications"}
{"qid": "hard-0288", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: A Theory of Multimodal Learning. Options: Miscellaneous Aspects of Machine Learning, Reinforcement Learning, Probabilistic Methods.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0289", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Towards Label Position Bias in Graph Neural Networks. Options: Reinforcement Learning, Deep Learning, Social Aspects.", "answer": "Deep Learning"}
{"qid": "hard-0290", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation. Options: Deep Learning, Optimization, Applications.", "answer": "Applications"}
{"qid": "hard-0291", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis. Options: Reinforcement Learning, Deep Learning, Applications.", "answer": "Reinforcement Learning"}
{"qid": "hard-0292", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Actively Testing Your Model While It Learns: Realizing Label-Efficient Learning in Practice. Options: Theory, Social Aspects, Probabilistic Methods.", "answer": "Theory"}
{"qid": "hard-0293", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Robust Learning with Progressive Data Expansion Against Spurious Correlation. Options: Theory, Social Aspects, Deep Learning.", "answer": "Deep Learning"}
{"qid": "hard-0294", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Network Regression with Graph Laplacians. Options: Applications, Theory, Miscellaneous Aspects of Machine Learning.", "answer": "Miscellaneous Aspects of Machine Learning"}
{"qid": "hard-0295", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design. Options: Optimization, Applications, Social Aspects.", "answer": "Applications"}
{"qid": "hard-0296", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Credal Marginal MAP. Options: Deep Learning, Probabilistic Methods, Reinforcement Learning.", "answer": "Probabilistic Methods"}
{"qid": "hard-0297", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Percentile Criterion Optimization in Offline Reinforcement Learning. Options: Deep Learning, Probabilistic Methods, Reinforcement Learning.", "answer": "Reinforcement Learning"}
{"qid": "hard-0298", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. Options: Deep Learning, Social Aspects, Applications.", "answer": "Deep Learning"}
{"qid": "hard-0299", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Learning List-Level Domain-Invariant Representations for Ranking. Options: Social Aspects, Theory, Applications.", "answer": "Theory"}
{"qid": "hard-0300", "question_type": "13", "question": "Predict the best fit topic for the title of a NeurIPS paper, which is not present in the database: Diverse Community Data for Benchmarking Data Privacy Algorithms. Options: Deep Learning, Applications, Social Aspects.", "answer": "Social Aspects"}
